{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "528d1128",
   "metadata": {},
   "source": [
    "* Prediction errors can be decomposed into two main subcomponents of interest: error from bias, and error from variance. The tradeoff between a model's ability to minimize bias and variance is foundational to training machine learning models.\n",
    "* The error on the training data is important for model tuning, but what we really care about is how it performs on data we haven't seen before, called test data.\n",
    "* `High Bias` : if The test error is even higher than the train error! then the model is underperforming. \n",
    "* `High Variance`: let's train a model that predicts every point in our training data perfectly.Now our training error is zero. Unsurprisingly, our model is too complicated. We say that it overfits the data. Instead of learning the true trends underlying our dataset, it memorized noise and, as a result, the model is not generalizable to datasets beyond its training data. `Overfitting`refers to the case when a model is so specific to the data on which it was trained that it is no longer applicable to different datasets.\n",
    "* `Test Error Decomposition`: mean-squared error can be decomposed into three components: error due to bias, error to to variance, and error due to noise.\n",
    "\\begin{align}\n",
    "\\Large Error = Bias^2 + Variance + Noice\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67cd7ad6",
   "metadata": {},
   "source": [
    "* `Bias` represents the difference between the average prediction and the true value.\n",
    "\\begin{align}\n",
    "\\Large Bias^2 = [E(\\hat{f}(x))-f(x)]^2\n",
    "\\end{align}\n",
    "* $E(\\hat{f}(x))$ It refers to the average prediction after the model has been trained over several independent datasets. For underfit (low-complexity) models, the majority of our error comes from bias.\n",
    "\n",
    "* `variance` measures how much, on average, predictions vary for a given data point. It is the amount by which the model's predictions will change if you use a different training set.\n",
    "\\begin{align}\n",
    "\\Large Variance =  E[[E(\\hat{f}(x))-\\hat{f}(x)]^2]\n",
    "\\end{align}\n",
    "\n",
    "* In other words, we don’t want an underfit model, but we don’t want an overfit model either. We want something in between - something with enough complexity to learn learn the generalizable patterns in our data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
