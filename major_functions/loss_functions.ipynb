{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "93515825",
   "metadata": {},
   "source": [
    "## **Metrics from L-P Norm**\n",
    "Informally, a norm is a function that accepts as input a vector from our vector space V\n",
    " and spits out a real number that tells us how big that vector is. In order for a function to qualify as a norm, it must first fulfill some properties, so that the results of this metrization process kind of “make sense”. These properties are the following.\n",
    " * Positive Definite \n",
    " * Absolute scalable \n",
    " * Triangular Inequality \n",
    "\n",
    "\\begin{align}\n",
    "\\large MAE = \\frac{1}{m}\\sum^{m}_{i=1}|y_i -\\hat{y}_i| \\large\\\\\n",
    "\\large MSE = \\frac{1}{m}\\sum^{m}_{i=1}(y_i -\\hat{y}_i)^2 \\large\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e749e9",
   "metadata": {},
   "source": [
    "##  Entropy\n",
    "* In information theory, entropy is a measure of uncertainty or randomness in a set of data. The more unpredictable or random the data, the higher its entropy. \n",
    "    \n",
    "\\begin{align}\n",
    "\\large H(X) = -\\sum_{i=1}^{n} P(x_i) \\log_2(P(x_i))\n",
    "\\end{align}\n",
    "* Probability and surprice is inveserly related. \n",
    "* when the probability is 1, the surprice needs to be zero, for this reason we can not say that surprice is 1/p. So surprice is defined as $log(1/p)$.\n",
    "* Entropy can be defined as expected value of surprice. \n",
    "* In summary, the entropy is a measure of the average amount of surprise or information content associated with a set of outcomes. If all outcomes are equally likely, the entropy is maximized, indicating maximum uncertainty. If some outcomes are more likely than others, the entropy is reduced, reflecting a lower level of uncertainty.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b2ff40",
   "metadata": {},
   "source": [
    "## Cross Entropy Loss\n",
    "\n",
    "Cross -entropy loss or log loss measures the performance of a classification model whose output is a probability value between 0 and 1. Cross entropy loss increases as the predicted probability diverges from the actual label.\n",
    "In binary classification, where the number of classes is two, cross entropy can be calculated as follows ;\n",
    "\n",
    "\\begin{align}\n",
    "\\large-(y*log(p) + (1-y) * log(1-p))\\LARGE\n",
    "\\end{align}\n",
    "\n",
    "In multi-classification scenario, we calculate separate loss for each class label per observation and sum the result. \n",
    "\n",
    "\\begin{align}\n",
    "\\large    -\\sum_{c=1}^{M} y*log(p) \\LARGE\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffe7814",
   "metadata": {},
   "source": [
    "## Huber Loss\n",
    "\n",
    "\\begin{cases} \n",
    "\\large\\frac{1}{2}(y - f(x))^2 & \\large\\text{for } |y - f(x)| \\leq \\delta \\\\\n",
    "\\large\\delta \\cdot (|y - f(x)| - \\frac{1}{2}\\delta) & \\large\\text{otherwise}\n",
    "\\end{cases}\n",
    "\n",
    "The Huber loss is more robust to outliers than MSE because it reduces the impact of large errors on the loss. It avoids the large errors squared in the quadratic term that can dominate the loss in the presence of outliers, making it more resistant to the influence of data points with extremely high residuals.\n",
    "The choice of the $\\delta\\ $ parameter depends on the specific characteristics of the data and the desired balance between the properties of MSE and MAE. \n",
    "A smaller $\\delta\\ $ makes the loss more quadratic, while a larger $\\delta\\$ makes it more linear.\n",
    "\n",
    "In summary, Huber loss is a compromise loss function that combines the advantages of both mean squared error and mean absolute error, providing a good balance between sensitivity to outliers and differentiability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861fdb79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8feb89b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1b1d88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7742bafd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
